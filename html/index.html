<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>Home</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Bootstrap -->
    <link href="../css/bootstrap.css" rel="stylesheet"/>
    <link rel="stylesheet" href="../css/flickity.css" media="screen">
    <link href="../css/style.css" rel="stylesheet"/>
    <link href='https://fonts.googleapis.com/css?family=Poiret+One' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Kaushan+Script' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Bree+Serif' rel='stylesheet' type='text/css'>
    
</head>

<body>
	
	
	<div class="splash">
		<img src = "../img/Logo.png" class="logo"/>
	</div>
	
	<div class="carousel js-flickity ">
		<div class="item1">
			
			<div class="contentBar">
			</div>
			<h1 class="header1">Concept</h1>
			<div class="paraConcept">
				<p>Audioscape is a concept for a mobile companion application which caters to 
				the needs of blind/visually impaired visitors of Te Papa museum. The concept 
				for the application involves the combination of GPS tracking technology and a 
				specially designed 3d sound environment which will allow it’s users to travel 
				through the museum environment whilst listening to audio content that is 
				responsive to their location within the museum. The user will hear location
				 specific ambient audio as they travel through the museum. The user can also 
				 engage with audio content associated with specific elements of exhibitions 
				 via image recognition technology. The intention is for the app to deliver 
				 museum content to blind/visually impaired users in a way that creates an 
				 immersive, interactive, museum environment, through using sound/audio effects
				  to flesh out a sense of space.
				</p>
			</div>
		</div>
	
		<div class="item2">
			<h1 class="userNeeds">User Needs</h1>
			
			<div class="points">
			<div class="needsText col-lg-6 col-md-6 col-xs-12">
				<h3 class="bree">Immersive</h3>
				<img src="../img/immerse.png"/>
				<p>One of the key experiential goals for this application is the creation 
				of an immersive museum environment for blind/visually impaired users. As a 
				very visual space, blind/visually impaired users are restricted in terms of
				 how fully they can engage and interact with the museum environment. It is 
				 therefore important for this application to facilitate a sense of immersion
				  and space that will enable users to feel like they are engaging with museum 
				  content in a more active way.

				</p>
			</div>
			
			<div class="needsText col-lg-6 col-md-6 col-xs-12">
				<h3 class="bree">Accessible</h3>
				<img src="../img/access.png"/>
				<p>As the application can not rely on visual cues, the app has to be 
				designed and structured in a way that is simple, intuitive  and easy 
				to navigate. In order to achieve this the application has to be stripped
				 down to a select few core features and interactions. This is critical 
				 as an application of this nature can’t rely on visual cues to guide 
				 user interaction, instead having to communicate with it’s users via 
				 audio feedback which is far more limited in the amount of information
				  that can be communicated at one given time.
 
				</p>
			</div>
			
			<div class="needsText col-lg-6 col-md-6 col-xs-12">
				<h3 class="bree">Intuitive</h3>
				<img src="../img/intuitive.png"/>
				<p>Another important component for this application is interface design. 
				As blind/visually impaired users can’t engage with standard user interfaces,
				 the interface design of this application needs to be created in such a way
				  that accommodates the specific needs of blind/visually impaired users. 
				  The application needs to utilise non-visual methods of interaction and 
				  user feedback as the primary forms of user engagement. Ideal interaction
				   methods would be minimal, simple, gestural movements.

				</p>
			</div>
			
			<div class="needsText col-lg-6 col-md-6 col-xs-12">
				<h3 class="bree">Interactive</h3>
				<img src="../img/interact.png"/>
				<p>It is a key part of the application to facilitate a high degree of 
				interactivity between user and museum environment/content. To achieve 
				this the application will have to be highly responsive to the user’s 
				movements and location. The user should be able to identify their 
				location based upon the audio they hear, and determine their distance
				 relative to another location based on how far away said location sounds.
				  The app should open up new channels of interactivity for blind/visually 
				  impaired users, thereby facilitating a greater sense of agency when 
				  engaging with the museum environment.
				</p>
			</div>
			
			</div>
		</div>	
	
		<div class="item3">
			<h1 class="techHead">Technological Requirements</h1>
			<div class="techContainer col-lg-5 col-md-5 col-sm-5 col-xs-10">
				<p class="techText">The core of the application will be constructed using HTML5, 
				Javascript, CSS and JQuery which will be used for the basic foundations of the 
				application, user interactivity and cross device responsiveness. The application 
				will be primarily utilised on tablet devices, and HTML5 and JQuery offer a number
				 of apis, plugins and libraries which will enable the application to fully exploit 
				 the interactive and technological capabilities of tablet/mobile devices. HTML 
				 offers both GPS and gyroscope apis while there are many JQuery libraries and 
				 plugins which enable touch responsive interactivity. 
				</p>
				<p class="techText">The application also involves more complex technologies such 
				as augmented reality/image recognition and 3D sound design. As I am unfamiliar 
				with these technologies, they may pose some difficulties during development. 
				In addition, sound files may have to be hosted externally and accessed over the
				 internet as having the files locally on the device may consume a considerable
				  amount of memory.

				</p>
			</div>
			<img src="../img/techPhone.png" class="techImage1"/>
			<img src="../img/techTab.png" class="techImage2"/>
		</div>
		
		<div class="item4">
			<h1 class="sensors">Sensors</h1>
			<div class="gpsText col-lg-5 col-md-4 col-sm-10 col-xs-10">
				<h3 class="senseHead">GPS</h3>
				<img src="../img/gps.png"/>
				<p>GPS will be the primary sensor for this application. The application will 
				detect the user position and track any changes/movements that have been made. 
				The application will have audio associated with certain areas, and will use 
				this location data to determine what audio content should be played. The 
				application will require highly specific GPS data and will therefore most 
				likely require an indoor GPS system.</p>
			</div>
			
			<div class="gyroText col-lg-5 col-md-4 col-sm-10 col-xs-10">
				<h3 class="senseHead">Gyroscope</h3>
				<img src="../img/gyro.png"/>
				<p>The gyroscope would be useful for not only the facilitation of a 
				tilt method of interaction, but also for detecting the presence of museum
				 items that have audio content associated with them. The application could
				  use a combination of GPS positioning and gyroscope bearings to determine 
				  if a user is pointing their tablet at an item with audio content associated
				   with it, and then proceed to deliver said content.</p>
			</div>
			
			<div class="arText col-lg-5 col-md-4 col-sm-10 col-xs-10">
				<h3 class="senseHead">Augmented Reality</h3>
				<img src="../img/ar.png"/>
				<p>An alternative detection method for museum artefacts would be the
				 use of augmented reality and image recognition technology. Users 
				 could use the camera on the tablet to scan the environment, which 
				 would then detect the presence of any museum items with audio content
				  associated with them. The application could potentially aid those who
				   are partially sighted, by allowing such users to aim the tablet at 
				   certain items and examine them close up through the tablet.</p>
			</div>
		</div>
		
		<div class="item5">
			
			<h1 class="DevPriHead">Device Prioritisation</h1>
			
			<div class="deviceContain">
				<div class="tablet col-lg-3 col-md-3 col-sm-3 col-xs-10">
					<img src="../img/tablet.png"/>
					<h3 class="deviceHead">Tablet</h3>
					<p>The tablet would be the primary platform for the application as it can 
					accommodate all the core technological requirements for the app, while also 
					providing a large enough platform for ease of interactivity. The tablet 
					accommodates all the core sensor technology as well as facilitates a great
					 range of non visual interaction methods that are necessary to the success 
					 of this project. Therefore the development decisions for this project will
					  be made in a way that prioritises the user experience on the tablet. 
					</p>
				</div>
			
				<div class="phone col-lg-3 col-md-3 col-sm-3 col-xs-10">
					<img src="../img/phone.png"/>
					<h3 class="deviceHead">Phone</h3>
					<p>The application could potentially be made available on mobile phones - 
					however the devices don’t possess the same size advantage that tablets 
					offer which better accommodates the gestural, non-visual methods of 
					interaction. However phones not only will possess most of the necessary 
					technology for the application, but are also more ubiquitous and subsequently 
					more accessible. A mobile phone version of the application could be made 
					available, however development will prioritise the tablet.</p>
				</div>
			
				<div class="comp col-lg-3 col-md-3 col-sm-3 col-xs-10">
					<img src="../img/computer.png"/>
					<h3 class="deviceHead">Computer</h3>
					<p>Computers do not possess the mobility nor the sensor technology necessary 
					for this application to function. Content associated with this application
					could perhaps be made available through a website, which could offer information 
					on the application as well as download links for the tablet and smartphone.</p>
				</div>
			</div>
		
		</div>
		
		<div class="item6">
			<img src="../img/wireframe1.png" class="appPlan"/>
			<h1 class="appHeader">Wireframe - home</h1>
		</div>
		
		<div class="item7">			
			<img src="../img/wireframe2.png" class="appPlan"/>
			<h1 class="appHeader">Wireframe - camera</h1>
		</div>
		
		<div class="item8">
			<img src="../img/wireframe3.png" class="appPlan"/>
			<h1 class="appHeader">Wireframe - detect</h1>
		</div>
		
		<div class="item9">
			<img src="../img/wireframe4.png" class="appPlan"/>
			<h1 class="appHeader">Wireframe - info</h1>
		</div>
		
		<div class="item10">
			<div class="contentBar">
			</div>
			<h1 class="futureHeader">Interaction Design</h1>
			<div class="paraConcept">
				<p>The primary users of this application will be blind/visually impaired.
				 Also, by extension, those afflicted by blindness or visual impairments 
				 tend to be on average, older than the general population. For both these 
				 reasons the key focus of interaction design for this application is simple,
				  intuitive, non-visual interaction. The application has been broken in to 
				  3 components - tutorial, general exploration mode and information mode. 
				  There are 4 core methods of engagement - tap(select), swipe(dismiss), 
				  tilt(scroll through options) and pinch(exit to main menu). These core 
				  interactions were designed to be minimal and have gestures that felt 
				  naturally associated with their functions. The main forms of user 
				  feedback is vibration and audio. Vibration is used for simple feedback
				   messages such as item selection or detection and audio is used to
				    communicate more specific information.</p>
			</div>
		</div>
		
		<div class="item11">
			<h1 class="appHeader2">Interaction Flow</h1>
			<img src="../img/Interaction1.jpg" class="appPlan2"/>
		</div>
		
		<div class="item12">
			<h1 class="appHeader2">Interaction Flow cont.</h1>
			<img src="../img/Interaction2.jpg" class="appPlan2"/>
		</div>
		
		<div class="item13">
			<div class="contentBar">
			</div>
			<h1 class="futureHeader">Future Directions</h1>
			<div class="paraConcept">
				<p>There are a number of opportunities for expansion/consideration in this project. One 
				area I wish to explore further are alternative methods of non-visual interaction 
				that could be involved in this application. For instance, voice activated
				 interaction could provide a number of different opportunities for exploration 
				 in terms of non-visual user interaction. Another thing to consider is the method
				  through which the application detects objects with audio content attached to them.
				   Is augmented reality/image recognition the best method for this application, or 
				   would something like the use of RFID tags be more efficient? Is there a way to 
				   enable blind/visually impaired users to easily come across these tags through 
				   this application? Another area to explore is a way to integrate visual content 
				   for the partially sighted user. Is there any method through which this application
				    could assist as a visual aid?</p>
			</div>
		</div>
		
	</div>

	<script src="../javascript/jquery-2.22.2.min.js" type="text/javascript"></script>
	<script src="../javascript/bootstrap.min.js"></script>
	<script src="../javascript/flickity.pkgd.min.js"></script>
	<script src="../javascript/script.js" type="text/javascript"></script>
	
</body>
</html>